{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Klasifikasi kegagalan sistem.ipynb",
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOFWJwaHbxfI3q8h1sNroWB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/denidiana2020/Machine_Learning/blob/main/Klasifikasi_kegagalan_sistem.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uhm_8vpOeEFV"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SmIKDM8eWQE"
      },
      "source": [
        "# Klasifikasi kegagalan sistem"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_f_buXbeR0j"
      },
      "source": [
        "\n",
        "Sebagai pelatihan tema praktis yang baik menggunakan pembelajaran mesin, Mengidentifikasi dan memprediksi penyebab kegagalan sistem tekanan udara.\n",
        "\n",
        "Notebook ini melakukan kegagalan sistem klasifikasi oleh LGBM\n",
        "EDA dan pra-pemrosesan data dikonfirmasi oleh kepentingan variabel dengan hutan acak dan pengelompokan dan PCA. Akibatnya, terapkan metode yang tampaknya baik untuk data latih ke data uji dan konfirmasi.\n",
        "Selain itu, untuk ketidakseimbangan data, konfirmasi efeknya dengan metode seperti oversampling dan undersampling."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sR2KhkDegF8"
      },
      "source": [
        "# Import Library"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q0ddrUcAeaIL"
      },
      "source": [
        "#library Sederhana\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "import warnings\n",
        "warnings.simplefilter(\"ignore\")\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use(\"fivethirtyeight\")\n",
        "import seaborn as sns\n",
        "\n",
        "# statiscics\n",
        "import scipy\n",
        "\n",
        "# Data preprocessing\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Grid search\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# StratifiedKFold\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "# Learning curve\n",
        "from sklearn.model_selection import learning_curve\n",
        "\n",
        "# Validation curve\n",
        "from sklearn.model_selection import validation_curve\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "from scipy import interp\n",
        "\n",
        "# Dimension reduction\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Classification method\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "import lightgbm as lgb\n",
        "\n",
        "# Imbalanced data preprocessing\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import NearMiss\n",
        "\n",
        "# Validation\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, roc_auc_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJiWT436fA20"
      },
      "source": [
        "# Dataloading dan cek Basis data¶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZ1YcUCJfOEz"
      },
      "source": [
        "content\n",
        "Set pelatihan berisi total 60000 contoh di mana 59000 termasuk dalam kelas negatif dan 1000 kelas positif. Set tes berisi 16000 contoh. Ada 171 atribut per record.\n",
        "\n",
        "Nama atribut data telah dianonimkan karena alasan kepemilikan. Ini terdiri dari penghitung numerik tunggal dan histogram yang terdiri dari tempat sampah dengan kondisi berbeda. Biasanya histogram memiliki kondisi ujung terbuka di setiap ujungnya. Misalnya, jika kita mengukur suhu lingkungan \"T\" maka histogram dapat didefinisikan dengan 4 bin di mana:\n",
        "\n",
        "Atributnya adalah sebagai berikut: kelas, kemudian data operasional yang dianonimkan. Data operasional memiliki pengenal dan id bin, seperti \"Identifier_Bin\". Total ada 171 atribut, dimana 7 adalah variabel histogram. Nilai yang hilang dilambangkan dengan \"na\".\n",
        "\n",
        "Ucapan Terima Kasih\n",
        "File ini adalah bagian dari Kegagalan APS dan Data Operasional untuk Truk Scania. Itu diimpor dari Repositori UCI ML.\n",
        "\n",
        "Inspirasi\n",
        "Total biaya model prediksi jumlah Biaya_1 dikalikan dengan jumlah Instance dengan kegagalan tipe 1 dan Biaya_2 dengan jumlah instans dengan kegagalan tipe 2, menghasilkan Total_cost. Dalam hal ini Biaya_1 mengacu pada biaya pemeriksaan yang tidak perlu yang perlu dilakukan oleh mekanik di bengkel, sedangkan Biaya_2 mengacu pada biaya kehilangan truk yang rusak, yang dapat menyebabkan kerusakan. Biaya_1 = 10 dan Biaya_2 = 500, dan Total_cost = Cost_1No_Instances + Cost_2No_Instances."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "id": "pTSsZZ3XftYx",
        "outputId": "0d5b8c2d-eae1-4544-a4c9-2b7f2dcdf0f3"
      },
      "source": [
        "#upload file dataset training dan test\n",
        "%cd sample_data/\n",
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Errno 2] No such file or directory: 'sample_data/'\n",
            "/content/sample_data\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4e7bbb88-e726-4fec-bc27-d5991b555970\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4e7bbb88-e726-4fec-bc27-d5991b555970\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8CMfoNhgJ-o"
      },
      "source": [
        "!ls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ep023Mszflq5"
      },
      "source": [
        "# train data\n",
        "train = pd.read_csv(\"/content/sample_data/aps_failure_training_set.csv\")\n",
        "# test data\n",
        "test = pd.read_csv(\"/content/sample_data/aps_failure_test_set.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SX3KOUX4giTM"
      },
      "source": [
        "# datacheck\n",
        "train.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "he6ELE57gsMX"
      },
      "source": [
        "# data size\n",
        "train.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N3Uco8kAgzL2"
      },
      "source": [
        "# null value\n",
        "train.isnull().sum().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CWguHIveg0H1"
      },
      "source": [
        "# data info\n",
        "train.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKSpR5zkg837"
      },
      "source": [
        "#Data prepcocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SewArXtRmYr0"
      },
      "source": [
        "Untuk sementara, 'na' berubah menjadi nilai Null.\n",
        "Setelah memahami seluruh data, pertimbangkan pemrosesan nilai nol."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2whr8cW7g_XM"
      },
      "source": [
        "# train data\n",
        "train = pd.read_csv(\"/content/sample_data/aps_failure_training_set.csv\", na_values=\"na\")\n",
        "# test data\n",
        "test = pd.read_csv(\"/content/sample_data/aps_failure_test_set.csv\",na_values=\"na\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ulydGr0Bmdkd"
      },
      "source": [
        "# null check\n",
        "col = train.iloc[:,1:].columns\n",
        "null_ratio = train.iloc[:,1:].isnull().sum().values / train.shape[0]*100"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDKJObYPmrQ7"
      },
      "source": [
        "# Check by visualization\n",
        "plt.figure(figsize=(20,8))\n",
        "plt.plot(col, null_ratio)\n",
        "plt.xlabel(\"variables\")\n",
        "plt.ylabel(\"ratio(%)\")\n",
        "plt.xticks(rotation=90, fontsize=10)\n",
        "plt.title(\"Null ratio\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WmfLnlX3mvBS"
      },
      "source": [
        "Beberapa variabel memiliki persentase nilai Null yang sangat tinggi. Bagaimana cara menggunakan informasi ini? Perlu pertimbangan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbsJmUENmz0R"
      },
      "source": [
        "# data type check\n",
        "train.dtypes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1fclthq9m5Te"
      },
      "source": [
        "# EDA\n",
        "Pemeriksaan jumlah kelas"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3LPs4P9bm_7K"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "sns.countplot(train[\"class\"])\n",
        "plt.title(\"neg:{0} / pos:{1}\".format(train[\"class\"].value_counts()[0], train[\"class\"].value_counts()[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fgRapE9cnJu6"
      },
      "source": [
        "Periksa distribusi variabel secara numerik\n",
        "Jumlah variabel adalah 171 yang sangat besar, jadi periksa dulu distribusi variabel secara numerik untuk mendapatkan karakteristiknya.\n",
        "\n",
        "Ganti label kelas dengan angka untuk analisis nanti. 1: positif, 0: negatif."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zGju3wl9nS-i"
      },
      "source": [
        "# define function\n",
        "def class_flg(x):\n",
        "    if x[\"class\"] == 'pos':\n",
        "        res = 1\n",
        "    else:\n",
        "        res = 0\n",
        "    return res\n",
        "\n",
        "train[\"class\"] = train.apply(class_flg, axis=1)\n",
        "# test data\n",
        "test[\"class\"] = test.apply(class_flg, axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3qv6zHdnbg5"
      },
      "source": [
        "Memahami distribusi dan hubungan setiap data¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iixS5jhoneKI"
      },
      "source": [
        "# min and max and mean and std\n",
        "col = train.iloc[:,1:].columns\n",
        "mean = train.iloc[:,1:].mean()\n",
        "min_ = train.iloc[:,1:].min()\n",
        "max_ = train.iloc[:,1:].max()\n",
        "std = train.iloc[:,1:].std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ptqdeOhjniTJ"
      },
      "source": [
        "# Visualization by plot\n",
        "plt.figure(figsize=(25,8))\n",
        "\n",
        "plt.plot(col, mean, linewidth=5, color=\"blue\", label='mean') #mean\n",
        "plt.fill_between(col, mean+std, mean-std, alpha=0.15, color='blue', label='±1σ') # ±1σ\n",
        "plt.plot(col, min_, linewidth=5, color='red', linestyle='--', label='max-min') # min\n",
        "plt.plot(col, max_, linewidth=5, color='green', linestyle='--') # max\n",
        "plt.xlabel(\"variables\")\n",
        "plt.ylabel(\"values\")\n",
        "plt.yscale(\"log\")\n",
        "plt.xticks(rotation=90, fontsize=10)\n",
        "plt.title(\"variables range\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAb6oukOnt25"
      },
      "source": [
        "Ini bukan grafik yang terorganisir, tetapi Anda dapat melihat ukuran dan lebar variabel. Nilai absolut dari setiap nilai dapat berkisar dari angka yang cukup besar hingga yang kecil. Bahkan jika Anda menggunakan grafik logaritmik, grafiknya telah berubah secara signifikan, jadi berhati-hatilah saat menangani setiap skala."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iN6KFqdnz3H"
      },
      "source": [
        "**Perbedaan menurut label**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OwtaLlyanzRf"
      },
      "source": [
        "train_mean = train.groupby(\"class\").mean().T\n",
        "train_std = train.groupby(\"class\").std().T\n",
        "\n",
        "# Visualization by plot\n",
        "plt.figure(figsize=(25,8))\n",
        "\n",
        "plt.plot(train_mean.index, train_mean[1], linewidth=5, color=\"red\", label='pos') #mean\n",
        "plt.fill_between(train_mean.index, train_mean[1]+train_std[1], train_mean[1]-train_std[1], alpha=0.15, color='orange', label='±1σ') # ±1σ\n",
        "\n",
        "plt.plot(train_mean.index, train_mean[0], linewidth=5, color=\"blue\", label='nag') #mean\n",
        "plt.fill_between(train_mean.index, train_mean[0]+train_std[0], train_mean[0]-train_std[0], alpha=0.15, color='green', label='±1σ') # ±1σ\n",
        "\n",
        "plt.xlabel(\"variables\")\n",
        "plt.ylabel(\"values\")\n",
        "plt.yscale(\"log\")\n",
        "plt.xticks(rotation=90, fontsize=10)\n",
        "plt.title(\"variables range\")\n",
        "plt.legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HONHb7DIoJm_"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PykuRUQ8oucx"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNO-Td-coQT9"
      },
      "source": [
        "# Correlation\n",
        "Next, check the correlation of each variable with a heat map"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T3tASK_ToYFi"
      },
      "source": [
        "# Null data are tempolary filled mean value.\n",
        "matrix = train.iloc[:,1:].iloc[1:].corr()\n",
        "plt.figure(figsize=(15,15))\n",
        "sns.heatmap(matrix, vmax=1, vmin=-1, cmap='bwr', square=True, annot=False, center=0, yticklabels=False, xticklabels=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "627rzrm6oj9N"
      },
      "source": [
        "Terlihat dari banyaknya variabel yang memiliki korelasi yang tinggi. Jika isi variabel dan hubungan teknisnya jelas, variabel dapat dipilih, tetapi kali ini tidak ada informasi sebelumnya. Oleh karena itu, perlu dilakukan ekstraksi variabel yang relevan dengan menggunakan analisis data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vj_BXR3yooBr"
      },
      "source": [
        "# Skerness and kurtosis\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LWXOYfmpok8v"
      },
      "source": [
        "# skew\n",
        "col = train.iloc[:,1:].columns\n",
        "# Roop, calculate with drop na values.\n",
        "skew = []\n",
        "for i in col:\n",
        "    sk = scipy.stats.skew(train[i].dropna())\n",
        "    skew.append(sk)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCsHyHIMopna"
      },
      "source": [
        "# kurtosis\n",
        "# Roop, calculate with drop na values.\n",
        "kurt = []\n",
        "for i in col:\n",
        "    ku = scipy.stats.kurtosis(train[i].dropna())\n",
        "    kurt.append(ku)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NcCUR72IovGm"
      },
      "source": [
        "# check with graph\n",
        "fig, ax = plt.subplots(1, 2, figsize=(20,6))\n",
        "sns.distplot(skew, ax=ax[0], kde=False, bins=100)\n",
        "ax[0].set_xlabel(\"Skewness\")\n",
        "ax[0].set_ylabel(\"Frequency\")\n",
        "ax[0].set_title(\"Skewness\")\n",
        "sns.distplot(kurt, ax=ax[1], kde=False, bins=100)\n",
        "ax[1].set_xlabel(\"Kurtosis\")\n",
        "ax[1].set_ylabel(\"Frequency\")\n",
        "ax[1].set_title(\"Kurtosis\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AAG_IqAuo1xE"
      },
      "source": [
        "Distribusi data dikonfirmasi. Pertama, hanya ditemukan nilai skewness yang lebih besar dari 0, dan sebarannya memiliki ekor kanan yang panjang dan bias ke kiri. Dikonfirmasi juga bahwa semua nilai sangat besar dan biasnya kuat. Terlihat banyak kurtosis yang mendekati 0 dan tidak berdistribusi tajam, namun beberapa kurtosis memiliki data dengan sebaran tajam yang besar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HIcN6m93pLgj"
      },
      "source": [
        "# Pendekatan untuk mengekstrak fitur penting (important Feature)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46FZjAhIpY85"
      },
      "source": [
        "**Direction**\n",
        "\n",
        "Untuk mencari fitur penting dari sejumlah besar variabel, analisis dilakukan dengan menggunakan tiga pendekatan berikut.\n",
        "1) PCA dan analisis klaster\n",
        "2) Pemeriksaan kepentingan oleh pengklasifikasi hutan acak\n",
        "\n",
        "Dari hasil ini, kami memutuskan untuk membuat penilaian yang komprehensif dan memutuskan fitur-fitur penting.\n",
        "\n",
        "Oleh karena itu, pertama-tama kami melakukan standarisasi data dan mengkonfirmasi distribusinya, terutama untuk yang memiliki rasio nol melebihi 15%. Melihat distribusi, kami mempertimbangkan variabel yang datanya memiliki varians dan kemungkinan ada jumlah sampel yang cukup.\n",
        "\n",
        "Adapun nilai-nilai Null lainnya, nilai-nilai dengan kepentingan yang tersisa dalam analisis kepentingan variabel berikutnya diadopsi, dan diputuskan untuk mengisinya dengan nilai rata-rata yang memiliki pengaruh statistik paling kecil."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uEMqzNJ6pVz-"
      },
      "source": [
        "# Data\n",
        "label = train[\"class\"]\n",
        "X = train.iloc[:,1:]\n",
        "col = train.iloc[:,1:].columns\n",
        "\n",
        "# Scaling\n",
        "# Create instance\n",
        "sc = StandardScaler()\n",
        "# Fitting\n",
        "sc.fit(X)\n",
        "# Transform\n",
        "X_std = sc.fit_transform(X)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_UsLM7wpzH8"
      },
      "source": [
        "## test data\n",
        "X_test_std = sc.fit_transform(test.iloc[1:])\n",
        "Y_test = test[\"class\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ucg2UFrbp1ad"
      },
      "source": [
        "# Create data frame\n",
        "train_std = pd.DataFrame(X_std, columns=col)\n",
        "train_std[\"class\"] = label\n",
        "train_std.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9EL5xTehp4lN"
      },
      "source": [
        "# Create test data frame\n",
        "test_std = pd.DataFrame(X_test_std, columns=test.iloc[1:].columns)\n",
        "test_std[\"class\"] = Y_test\n",
        "test_std.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nN0jr2Iap9ac"
      },
      "source": [
        "# Null value\n",
        "null_df = pd.DataFrame({\"variables\":train.iloc[:,1:].columns,\n",
        "                        \"null_ratio\":null_ratio})\n",
        "null_over15_col = null_df[null_df[\"null_ratio\"]>15][\"variables\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M5t3V1uuqAQm"
      },
      "source": [
        "# Result columns\n",
        "null_over15_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e-b5Wvd9qFCj"
      },
      "source": [
        "# null ratio df\n",
        "null_ratio_df = pd.DataFrame({\"variables\":train_std[null_over15_col].isnull().sum().index, \n",
        "                             \"null_ratio\":train_std[null_over15_col].isnull().sum()/len(train_std)*100}).sort_values(by=\"null_ratio\", ascending=False)\n",
        "\n",
        "# over 15% null value df\n",
        "null_df = train_std[null_ratio_df[\"variables\"]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S0IBGlAZqJoL"
      },
      "source": [
        "# distribution check\n",
        "col = null_df.columns\n",
        "fig, ax = plt.subplots(4, 7, figsize=(25, 20))\n",
        "plt.subplots_adjust(hspace=0.5)\n",
        "for i in range(len(col)):\n",
        "    if i <= 6:\n",
        "        sns.distplot(null_df[col[i]], ax=ax[0,i], kde=False)\n",
        "        ax[0,i].set_title(\"Null ratio(%):\\n{}\".format(null_ratio_df[null_ratio_df[\"variables\"]==col[i]][\"null_ratio\"].values))\n",
        "    if i > 6 and i <= 13:\n",
        "        sns.distplot(null_df[col[i]], ax=ax[1,i-7], kde=False)\n",
        "        ax[1,i-7].set_title(\"Null ratio(%):\\n{}\".format(null_ratio_df[null_ratio_df[\"variables\"]==col[i]][\"null_ratio\"].values))\n",
        "    if i > 13 and i <= 20:\n",
        "        sns.distplot(null_df[col[i]], ax=ax[2,i-14], kde=False)\n",
        "        ax[2,i-14].set_title(\"Null ratio(%):\\n{}\".format(null_ratio_df[null_ratio_df[\"variables\"]==col[i]][\"null_ratio\"].values))\n",
        "    if i > 20 and i <= 27:\n",
        "        sns.distplot(null_df[col[i]], ax=ax[3,i-21], kde=False)\n",
        "        ax[3,i-21].set_title(\"Null ratio(%):\\n{}\".format(null_ratio_df[null_ratio_df[\"variables\"]==col[i]][\"null_ratio\"].values))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1I7E07AEqavs"
      },
      "source": [
        "Sebagai hasil dari konfirmasi distribusi, diputuskan untuk meninggalkan bl_000 dan bk_000 karena rasio nilai nol adalah 40 hingga 50% dan ada lebih dari setengah data, dan data juga tersebar dan kemungkinan memiliki informasi. .. Variabel lain dikeluarkan karena jumlah datanya kecil dan biasnya hampir satu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EaFREwlVqZwM"
      },
      "source": [
        "# Columns of drop\n",
        "drop_col = null_over15_col.values\n",
        "drop_col = np.delete(drop_col, np.where((drop_col == 'bl_000') & (drop_col == 'bk_000')))\n",
        "# Checking\n",
        "drop_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pd9hNOi3q4A6"
      },
      "source": [
        "# Drop over15% null columns, and fill mean values\n",
        "train_std.drop(drop_col, axis=1, inplace=True)\n",
        "\n",
        "# Roop fill mean\n",
        "for i in train_std.columns:\n",
        "    mean = train_std[i].mean()\n",
        "    train_std[i].fillna(mean, inplace=True)\n",
        "    \n",
        "train_std.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXtE1nM2q78T"
      },
      "source": [
        "# Drop over15% null columns, and fill mean values\n",
        "test_std.drop(drop_col, axis=1, inplace=True)\n",
        "\n",
        "# Roop fill mean\n",
        "for i in test_std.columns:\n",
        "    mean = test_std[i].mean()\n",
        "    test_std[i].fillna(mean, inplace=True)\n",
        "    \n",
        "test_std.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOjeOZxOrD7j"
      },
      "source": [
        "train_std.corr().isnull().sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4QlRSChurMH7"
      },
      "source": [
        "Beberapa data memiliki varians nol. Hilangkan karena tidak mempengaruhi hasil analisis dan menjadi risiko komputasi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zPtsYpXPrOPq"
      },
      "source": [
        "# Sumber kesalahan yang terjadi selama analisis\n",
        "train_std[\"cd_000\"].var()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ldzy2sckrWeu"
      },
      "source": [
        "train_std.drop(\"cd_000\", axis=1, inplace=True)\n",
        "# test data\n",
        "test_std.drop(\"cd_000\", axis=1, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "89R8XbINrZ1I"
      },
      "source": [
        "# Difine variables\n",
        "X = train_std.iloc[:,:-1]\n",
        "Y = train_std[\"class\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XG8M-SvXrcco"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yja-JAKnrehZ"
      },
      "source": [
        "# K-Means Clustering¶\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWzb9vairkI3"
      },
      "source": [
        "Periksa dengan nomor cluster, dengan metode siku."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L1Hu26yrlXv"
      },
      "source": [
        "# Calculation of distiortions\n",
        "distortions = []\n",
        "for i in range(1,20):\n",
        "    km = KMeans(n_clusters=i, init='k-means++', n_init=10, max_iter=100, random_state=10)\n",
        "    km.fit(X)\n",
        "    distortions.append(km.inertia_)\n",
        "    \n",
        "# Plotting \n",
        "plt.figure(figsize=(10,6))\n",
        "plt.plot(range(1,20), distortions, marker='o')\n",
        "plt.xlabel(\"Number of clusters\")\n",
        "plt.xticks(range(1,20))\n",
        "plt.ylabel(\"Distortion\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5n454TOUr11y"
      },
      "source": [
        "Dapat dilihat bahwa distorsi berkurang dengan bertambahnya jumlah cluster. Secara khusus, dapat dilihat bahwa jumlah cluster sangat berkurang menjadi 4 dan kemudian secara bertahap berkurang. Kali ini, kami memutuskan untuk melanjutkan analisis, dengan asumsi bahwa jumlah cluster di mana kurva ini asimtotik adalah hingga 8."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lgMl0jnBrqUI"
      },
      "source": [
        "# Clustering n=8\n",
        "kmeans = KMeans(n_clusters=8, init='k-means++', n_init=10, max_iter=100, random_state=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0sg2_TH-r7oI"
      },
      "source": [
        "# Fitting\n",
        "kmeans.fit(X)\n",
        "# output\n",
        "cluster = kmeans.labels_"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vFVVmRJ9r9ki"
      },
      "source": [
        "# test data fit transform and labels\n",
        "cluster_test = kmeans.fit_predict(test_std.iloc[:,:-1])\n",
        "print(cluster_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ewoz_TwPsC0i"
      },
      "source": [
        "# PCA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ErWTq0g9sQHm"
      },
      "source": [
        "Untuk menentukan jumlah komponen utama, kami mengkonfirmasi peluruhan nilai eigen dari varians dengan plot scree."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aq-IE7rqsZZH"
      },
      "source": [
        "**Scree plot**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HagUvDLcslS_"
      },
      "source": [
        "eigen_vals = sorted(np.linalg.eigvals(X.corr()), reverse=True)\n",
        "\n",
        "# plot\n",
        "fig, ax = plt.subplots(2, 1, figsize=(20,10))\n",
        "plt.subplots_adjust(hspace=0.4)\n",
        "ax[0].plot(eigen_vals, 's-')\n",
        "ax[0].set_xlabel(\"factor\")\n",
        "ax[0].set_ylabel(\"eigenvalue\")\n",
        "\n",
        "ax[1].plot(eigen_vals, 's-')\n",
        "ax[1].set_xlabel(\"factor\")\n",
        "ax[1].set_ylabel(\"eigenvalue\")\n",
        "ax[1].set_ylim([0,10])\n",
        "ax[1].set_title(\"Scale up\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc9BHDjTs6j4"
      },
      "source": [
        "Sebagai hasil dari konfirmasi nilai eigen menggunakan nilai varians, sangat mungkin bahwa bahkan dua dapat menjelaskan sebagian besar dari keseluruhan data, dan bahkan jika ada 10 variabel, seluruh data dapat diungkapkan dengan informasi yang cukup. Saya mengerti."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EODOlB6ns7-2"
      },
      "source": [
        "# Create instance, n=10\n",
        "pca = PCA(n_components=10)\n",
        "\n",
        "# Fitting\n",
        "pca_result = pca.fit_transform(X)\n",
        "pca_result = pd.DataFrame(pca_result, columns=[\"pca1\",\"pca2\",\"pca3\",\"pca4\",\"pca5\",\"pca6\",\"pca7\",\"pca8\",\"pca9\",\"pca10\"])\n",
        "\n",
        "pca_result.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wlh2JvVTtCEP"
      },
      "source": [
        "# Visualization by heatmap\n",
        "plt.figure(figsize=(20,20))\n",
        "sns.heatmap(pca.components_.T, vmax=1, vmin=-1, cmap='bwr', square=False, annot=False, center=0, yticklabels=X.columns, xticklabels=pca_result.columns)\n",
        "plt.xlabel(\"pca\")\n",
        "plt.ylabel(\"variables\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2erHk6JvtKhd"
      },
      "source": [
        "Meskipun hubungan jumlah fitur berbeda untuk setiap komponen utama, kekuatan variabel yang terkait dengan informasi data dapat dilihat dari gambar ini. Dapat juga dilihat bahwa selama ini terdapat variabel-variabel yang tidak berhubungan dengan komponen utama."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lD3Sp7v5tLQU"
      },
      "source": [
        "# Visualization by plot\n",
        "x = pca_result[\"pca1\"]\n",
        "y = pca_result[\"pca2\"]\n",
        "color = Y\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(x, y, c=color, alpha=0.5)\n",
        "plt.xlabel(\"PCA1\")\n",
        "plt.ylabel(\"PCA2\")\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uxG7h3ZtX5M"
      },
      "source": [
        "Sebagian besar dari dua sumbu komponen utama tumpang tindih ketika diplot oleh label kelas, tetapi daerah dengan sejumlah besar kelas positif dan daerah dengan kelas negatif padat dan padat dipisahkan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82XV8voStYsO"
      },
      "source": [
        "# create dataframe\n",
        "pca_result[\"class\"] = Y\n",
        "\n",
        "fig, ax = plt.subplots(1,2,figsize=(20,6))\n",
        "sns.distplot(pca_result[pca_result[\"class\"]==1][\"pca1\"], label=\"pos\", ax=ax[0])\n",
        "sns.distplot(pca_result[pca_result[\"class\"]==0][\"pca1\"], label=\"neg\", ax=ax[0])\n",
        "ax[0].legend()\n",
        "\n",
        "sns.distplot(pca_result[pca_result[\"class\"]==1][\"pca2\"], label=\"pos\", ax=ax[1])\n",
        "sns.distplot(pca_result[pca_result[\"class\"]==0][\"pca2\"], label=\"neg\", ax=ax[1])\n",
        "ax[1].legend()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aD1NdaRatgsl"
      },
      "source": [
        "Lebih mudah untuk memahami distribusi setiap sumbu komponen utama yang diplot. Mengenai sumbu pca1, kelas positif bias menuju nilai yang lebih besar, dan kelas negatif mengelompok di sekitar 0. Sumbu pca2 tersebar luas dari nilai negatif ke nilai positif di kelas positif, tetapi bias mendekati 0 di negatif kelas."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oBy9yjC4thn0"
      },
      "source": [
        "pca_result[\"cluster\"] = cluster\n",
        "\n",
        "# Visualization by plot\n",
        "x = pca_result[\"pca1\"]\n",
        "y = pca_result[\"pca2\"]\n",
        "\n",
        "plt.figure(figsize=(10,6))\n",
        "plt.scatter(x, y, c=cluster, alpha=0.5, cmap=\"Set1\")\n",
        "plt.xlabel(\"PCA1\")\n",
        "plt.ylabel(\"PCA2\")\n",
        "plt.colorbar()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OeE9TgAWtoRb"
      },
      "source": [
        "# pivot count \n",
        "pivot = pd.pivot_table(data=pca_result, index=\"class\", columns=\"cluster\", values=\"pca1\", aggfunc=\"count\", fill_value=0)\n",
        "pivot.columns = [\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\", \"c8\"]\n",
        "pivot.reset_index()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bHYZC1ePt2Ro"
      },
      "source": [
        "Selanjutnya, ketika hasil analisis klaster diplot, dipastikan bahwa klaster dapat dibagi bahkan pada sumbu komponen utama.\n",
        "Apakah cluster ini membagi kelas positif dan negatif untuk label kelas? dikonfirmasi. Akibatnya, cluster 1 dan cluster 5 dapat membentuk cluster yang memiliki sejumlah besar kelas negatif. Ini adalah titik tajam di sisi kiri grafik plot. (Catatan: area kuning dan merah di plot)\n",
        "\n",
        "Selanjutnya, saya juga mengkonfirmasi bagaimana variabel lain berbeda dalam cluster..\n",
        "Namun, cluster 8 jumlahnya kecil dan merupakan outlier, sehingga dikecualikan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvUfeQxYt3ly"
      },
      "source": [
        "# Characteristics of each cluster\n",
        "# Create dataframe\n",
        "cluster_name = [\"c1\", \"c2\", \"c3\", \"c4\", \"c5\", \"c6\", \"c7\"]\n",
        "pca_label = [\"pca1\",\"pca2\",\"pca3\",\"pca4\",\"pca5\",\"pca6\",\"pca7\",\"pca8\",\"pca9\",\"pca10\"]\n",
        "\n",
        "cluster_stats = pd.DataFrame({\"cluster\":range(0,7)})\n",
        "cluster_pos_mean = pd.merge(cluster_stats, pca_result[pca_result[\"class\"]==1].groupby(\"cluster\").mean()[pca_label].reset_index(),\n",
        "                            left_on=\"cluster\", right_on=\"cluster\", how=\"left\").drop(\"cluster\", axis=1)\n",
        "cluster_pos_std = pd.merge(cluster_stats, pca_result[pca_result[\"class\"]==1].groupby(\"cluster\").std()[pca_label].reset_index(),\n",
        "                            left_on=\"cluster\", right_on=\"cluster\", how=\"left\").drop(\"cluster\", axis=1)\n",
        "cluster_neg_mean = pd.merge(cluster_stats, pca_result[pca_result[\"class\"]==0].groupby(\"cluster\").mean()[pca_label].reset_index(),\n",
        "                            left_on=\"cluster\", right_on=\"cluster\", how=\"left\").drop(\"cluster\", axis=1)\n",
        "cluster_neg_std = pd.merge(cluster_stats, pca_result[pca_result[\"class\"]==0].groupby(\"cluster\").std()[pca_label].reset_index(),\n",
        "                            left_on=\"cluster\", right_on=\"cluster\", how=\"left\").drop(\"cluster\", axis=1)\n",
        "\n",
        "# Change name of columns\n",
        "cluster_pos_mean.index = cluster_name\n",
        "cluster_pos_std.index = cluster_name\n",
        "cluster_neg_mean.index = cluster_name\n",
        "cluster_neg_std.index = cluster_name"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMfSlHW9t927"
      },
      "source": [
        "# Visualization\n",
        "fig, ax = plt.subplots(2,5,figsize=(20,10))\n",
        "plt.subplots_adjust(hspace=0.5, wspace=0.4)\n",
        "\n",
        "for i in range(len(cluster_pos_mean.columns)):\n",
        "    if i <5:\n",
        "        ax[0,i].plot(cluster_pos_mean.index, cluster_pos_mean[pca_label[i]], color=\"red\", label=\"pos\")\n",
        "        ax[0,i].fill_between(cluster_pos_mean.index, cluster_pos_mean[pca_label[i]]+cluster_pos_std[pca_label[i]],\n",
        "                             cluster_pos_mean[pca_label[i]]-cluster_pos_std[pca_label[i]], color=\"orange\", alpha=0.3, label=\"±1σ\")\n",
        "        ax[0,i].plot(cluster_neg_mean.index, cluster_neg_mean[pca_label[i]], color=\"blue\", label=\"neg\")\n",
        "        ax[0,i].fill_between(cluster_neg_mean.index, cluster_neg_mean[pca_label[i]]+cluster_neg_std[pca_label[i]],\n",
        "                             cluster_neg_mean[pca_label[i]]-cluster_neg_std[pca_label[i]], color=\"green\", alpha=0.3, label=\"±1σ\")\n",
        "        ax[0,i].set_title(pca_label[i])\n",
        "        ax[0,i].set_xlabel(\"Cluster\")\n",
        "        ax[0,i].set_ylabel(\"Standarlized values\")\n",
        "        ax[0,i].tick_params(axis='x', labelrotation=90)\n",
        "        ax[0,i].legend(ncol=2)\n",
        "    else:\n",
        "        ax[1,i-5].plot(cluster_pos_mean.index, cluster_pos_mean[pca_label[i]], color=\"red\", label=\"pos\")\n",
        "        ax[1,i-5].fill_between(cluster_pos_mean.index, cluster_pos_mean[pca_label[i]]+cluster_pos_std[pca_label[i]],\n",
        "                               cluster_pos_mean[pca_label[i]]-cluster_pos_std[pca_label[i]], color=\"orange\", alpha=0.3, label=\"±1σ\")\n",
        "        ax[1,i-5].plot(cluster_neg_mean.index, cluster_neg_mean[pca_label[i]], color=\"blue\", label=\"neg\")\n",
        "        ax[1,i-5].fill_between(cluster_neg_mean.index, cluster_neg_mean[pca_label[i]]+cluster_neg_std[pca_label[i]],\n",
        "                               cluster_neg_mean[pca_label[i]]-cluster_neg_std[pca_label[i]], color=\"green\", alpha=0.3, label=\"±1σ\")\n",
        "        ax[1,i-5].set_title(pca_label[i])\n",
        "        ax[1,i-5].set_xlabel(\"Cluster\")\n",
        "        ax[1,i-5].set_ylabel(\"Standarlized values\")\n",
        "        ax[1,i-5].tick_params(axis='x', labelrotation=90)\n",
        "        ax[1,i-5].legend(ncol=2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7nqge22uUGk"
      },
      "source": [
        "Sebagai hasil dari konfirmasi setiap komponen utama selain pca1 dan pca2, karakteristik komponen utama diklasifikasikan untuk setiap cluster. Point of interest adalah cluster 1 dan 5 yang sebagian besar termasuk dalam kelas negatif tadi, namun dengan melihat masing-masing sumbu dapat diketahui bahwa cluster lain dapat memiliki nilai yang sama. Artinya tidak mungkin membuat suatu penilaian dengan hanya satu variabel saja, hal ini menunjukkan perlunya membuat suatu penilaian positif/negatif yang mencakup hubungan antar masing-masing variabel.\n",
        "\n",
        "Di sisi lain, cluster 4, 6 dan 7 adalah cluster yang memiliki sejumlah besar kelas positif, dan dapat dipastikan bahwa nilainya juga merupakan karakteristik. Faktanya, memahami karakteristik kluster ini akan membantu menganalisis mekanisme mekanis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEgGa_eCuaeh"
      },
      "source": [
        "# Feature importance analysis dengan menggunakan Random forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6u4Rd77us4O"
      },
      "source": [
        "Selanjutnya, kami menganalisis pentingnya fitur menggunakan pendekatan yang sama sekali berbeda. Prediksi klasifikasi dilakukan di hutan acak, dan pentingnya fitur diekstraksi menggunakan perolehan informasi yang merupakan fitur dari pohon keputusan.\n",
        "\n",
        "Ada masalah besar dalam mengklasifikasikan. Artinya, ada bias yang besar antara kelas positif dan negatif. Dalam kasus seperti itu, melengkapi menggunakan metode oversampling atau metode undersampling, dan menyeimbangkan setiap kelas untuk membuat model.\n",
        "Kali ini, hanya metode oversampling yang digunakan karena hanya fitur kuantitas fitur yang penting. Juga, untuk referensi, sebagai hasil dari analisis klaster, klaster yang memiliki banyak kelas negatif dapat diapit. Oleh karena itu, kami mencoba membangun model dengan sampel data tidak termasuk cluster.\n",
        "\n",
        "Untuk metode lain dari data yang tidak seimbang, efeknya diverifikasi ketika membangun model prediksi setelah analisis kepentingan ini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d99BlO62u2PD"
      },
      "source": [
        "**Konfirmasi pentingnya fitur, dengan metode oversampling**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BddxZLo3uiTb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ngXdE8TKu77X"
      },
      "source": [
        "# Over sampling method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hFVzTd0rvI5f"
      },
      "source": [
        "Eksekusi pengklasifikasi Random Forest"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MafWuxlIu_b4"
      },
      "source": [
        "# Difine variables\n",
        "X = train_std.iloc[:,:-1]\n",
        "Y = train_std[\"class\"]\n",
        "\n",
        "# Data preprocessing, oversampling method\n",
        "# create instance\n",
        "ros = RandomOverSampler(sampling_strategy=\"auto\", random_state=10)\n",
        "\n",
        "# train test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n",
        "\n",
        "# Apply to data\n",
        "X_resampled, y_resampled = ros.fit_sample(X_train, y_train)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hEcdA1zsvOm2"
      },
      "source": [
        "# Create instance\n",
        "forest  = RandomForestClassifier(n_estimators=10, random_state=10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3waxO8pvTEC"
      },
      "source": [
        "# parameters\n",
        "param_range = [10,15,20]\n",
        "leaf = [70, 75, 80, 85]\n",
        "criterion = [\"entropy\", \"gini\", \"error\"]\n",
        "param_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n",
        "\n",
        "# Optimization by Grid search, scoring is f1 score\n",
        "gs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\n",
        "gs = gs.fit(X_resampled, y_resampled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vi3T41c5vVQR"
      },
      "source": [
        "print(\"gs best:%.3f\" % gs.best_score_)\n",
        "print(\"gs params:{}\".format(gs.best_params_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VaQBBdjdvZDE"
      },
      "source": [
        "# Prediction\n",
        "gs_best = gs.best_estimator_\n",
        "\n",
        "y_pred = gs_best.predict(X_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7xjbEMJ9vbbi"
      },
      "source": [
        "#Scores\n",
        "print(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
        "print(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A8e8ONh4vknK"
      },
      "source": [
        "Hasil untuk data pelatihan memberikan ekstraksi yang solid dari kelas positif. Namun, skor presisi rendah karena seluruh data tidak seimbang. Meskipun ini merupakan masalah, dapat dievaluasi bahwa nilai recall tinggi dan kelas positif dapat diklasifikasikan untuk sementara waktu.\n",
        "Dari hasil model ini diperoleh derajat kepentingan jumlah fitur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5vStNd0OvpfB"
      },
      "source": [
        "# Create best model of random forest classifier\n",
        "forest  = RandomForestClassifier(n_estimators=10, random_state=10, criterion=\"entropy\", max_depth=15, max_leaf_nodes=60)\n",
        "forest.fit(X_resampled, y_resampled)\n",
        "\n",
        "importance = forest.feature_importances_\n",
        "\n",
        "indices = np.argsort(importance)[::-1]\n",
        "\n",
        "# Due to the large number of items, only the top 30 were written.\n",
        "for i in range(30):\n",
        "    print(\"%2d) %-*s %f\" %(i+1, 10, X.columns[indices[i]], importance[indices[i]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lm_sih5yvtV4"
      },
      "source": [
        "# Visualization with paret0 graph\n",
        "forest_importance1 = pd.DataFrame({})\n",
        "variables = []\n",
        "feature_importance1 = []\n",
        "for i in range(len(indices)):\n",
        "    col = X.columns[indices[i]]\n",
        "    impor = importance[indices[i]]\n",
        "    variables.append(col)\n",
        "    feature_importance1.append(impor)\n",
        "forest_importance1[\"variables\"] = variables\n",
        "forest_importance1[\"feature_importance1\"] = feature_importance1\n",
        "forest_importance1[\"feature_importance1\"] = forest_importance1[\"feature_importance1\"]*100\n",
        "forest_importance1[\"cumsum\"] = forest_importance1[\"feature_importance1\"].cumsum()\n",
        "\n",
        "\n",
        "# Graph\n",
        "fig, ax1 = plt.subplots(figsize=(20,8))\n",
        "ax1.bar(forest_importance1[\"variables\"], forest_importance1[\"feature_importance1\"], label=\"importance\")\n",
        "ax1.grid()\n",
        "ax1.set_xlabel(\"variables\")\n",
        "ax1.tick_params(axis='x', rotation=90, labelsize=10)\n",
        "ax1.set_ylabel(\"importance(%)\")\n",
        "plt.legend(loc='lower left')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(forest_importance1[\"variables\"], forest_importance1[\"cumsum\"], color=\"red\", label=\"ratio\")\n",
        "ax2.set_ylim([0,110])\n",
        "ax2.set_ylabel(\"Ratio(%)\")\n",
        "plt.legend(loc=\"upper right\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3GctFz9BvvtJ"
      },
      "source": [
        "Tentang hasil klasifikasi hutan secara acak ini, ditemukan bahwa sekitar 1/4 variabel dapat menjelaskan lebih dari 90% klasifikasi, dan 2/3 variabel dapat menjelaskan hampir 100%."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2npGnC-Yv19v"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xCJs2ipXwDv_"
      },
      "source": [
        "# Analisis kepentingan fitur dengan menggunakan hasil Random Forest dan Clustering\n",
        "\n",
        "Dari hasil analisis klaster diketahui bahwa kedua klaster tersebut hampir tidak mengandung label positif. Dengan menghapus cluster ini, jumlah kelas positif dan negatif didekatkan, dan analisis serupa dilakukan."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7YCDg4sqwIPR"
      },
      "source": [
        "train_std[\"cluster\"] = cluster\n",
        "# Create variable data and label data by removing the cluster data of c1 and c5 (0th and 4th in the label).\n",
        "\n",
        "# test data\n",
        "test_std[\"cluster\"] = cluster_test"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fTW7DNE_wUdV"
      },
      "source": [
        "# Difine variables\n",
        "X = train_std.query(\"cluster!=0 & cluster!=4\").iloc[:,:-2]\n",
        "Y = train_std.query(\"cluster!=0 & cluster!=4\")[\"class\"]\n",
        "\n",
        "# train test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n",
        "\n",
        "# Create instance\n",
        "forest  = RandomForestClassifier(n_estimators=10, random_state=10)\n",
        "\n",
        "# parameters\n",
        "param_range = [5, 10,15]\n",
        "leaf = [60, 65, 70, 75]\n",
        "criterion = [\"entropy\", \"gini\", \"error\"]\n",
        "param_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n",
        "\n",
        "# Optimization by Grid search, scoring is f1\n",
        "gs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\n",
        "gs = gs.fit(X_train, y_train)\n",
        "\n",
        "print(\"gs best:%.3f\" % gs.best_score_)\n",
        "print(\"gs params:{}\".format(gs.best_params_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HWnNh3EWwY7m"
      },
      "source": [
        "# Prediction\n",
        "gs_best = gs.best_estimator_\n",
        "\n",
        "y_pred = gs_best.predict(X_test)\n",
        "\n",
        "# Scores\n",
        "print(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
        "print(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cU2wBjSMyJLP"
      },
      "source": [
        "Hasil untuk data pelatihan memberikan ekstraksi yang solid dari kelas positif. Skor presisi 84%. Meskipun ini merupakan masalah, dapat dievaluasi bahwa nilai recall rendah dan kelas positif dapat diklasifikasikan untuk sementara waktu. Dari hasil model ini diperoleh derajat kepentingan jumlah fitur."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O5_QmZzAwb1r"
      },
      "source": [
        "# Create best model of random forest classifier\n",
        "forest  = RandomForestClassifier(n_estimators=10, random_state=10, criterion=\"entropy\", max_depth=15, max_leaf_nodes=60)\n",
        "forest.fit(X_train, y_train)\n",
        "\n",
        "importance = forest.feature_importances_\n",
        "\n",
        "indices = np.argsort(importance)[::-1]\n",
        "\n",
        "# Due to the large number of items, only the top 30 were written.\n",
        "for i in range(30):\n",
        "    print(\"%2d) %-*s %f\" %(i+1, 10, X.columns[indices[i]], importance[indices[i]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdWm1de5wfaN"
      },
      "source": [
        "# Visualization with paret0 graph\n",
        "forest_importance2 = pd.DataFrame({})\n",
        "variables = []\n",
        "feature_importance2 = []\n",
        "for i in range(len(indices)):\n",
        "    col = X.columns[indices[i]]\n",
        "    impor = importance[indices[i]]\n",
        "    variables.append(col)\n",
        "    feature_importance2.append(impor)\n",
        "forest_importance2[\"variables\"] = variables\n",
        "forest_importance2[\"feature_importance2\"] = feature_importance2\n",
        "forest_importance2[\"feature_importance2\"] = forest_importance2[\"feature_importance2\"]*100\n",
        "forest_importance2[\"cumsum\"] = forest_importance2[\"feature_importance2\"].cumsum()\n",
        "\n",
        "\n",
        "# Graph\n",
        "fig, ax1 = plt.subplots(figsize=(20,8))\n",
        "ax1.bar(forest_importance2[\"variables\"], forest_importance2[\"feature_importance2\"], label=\"importance\")\n",
        "ax1.grid()\n",
        "ax1.set_xlabel(\"variables\")\n",
        "ax1.tick_params(axis='x', rotation=90, labelsize=10)\n",
        "ax1.set_ylabel(\"importance(%)\")\n",
        "plt.legend(loc='lower left')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(forest_importance2[\"variables\"], forest_importance2[\"cumsum\"], color=\"red\", label=\"ratio\")\n",
        "ax2.set_ylim([0,110])\n",
        "ax2.set_ylabel(\"Ratio(%)\")\n",
        "plt.legend(loc=\"upper right\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuNWrit9wkKJ"
      },
      "source": [
        "Dari hasil tersebut, kita dapat melihat peningkatan nilai presisi. Di sisi lain, nilai recall mengalami penurunan, namun secara keseluruhan skornya baik. Distribusi derajat kepentingan variabel telah berubah dari hasil sebelumnya. Garis dengan lebih dari 90% memiliki hampir setengah jumlah variabel, dan asimtotik hingga 100% sekitar 4/5. Dengan kata lain, variabel yang memberikan hasil yang tidak begitu penting dengan pelatihan secara keseluruhan menjadi lebih penting pada cluster dengan konsentrasi kelas positif yang tinggi, dan menjadi variabel yang diperlukan untuk klasifikasi. Itu mungkin untuk meningkatkan kinerja klasifikasi dengan menggabungkan analisis cluster.\n",
        "Juga, jika ada variabel yang ditemukan tidak signifikan baik dalam hasil keseluruhan dan hasil ini, kemungkinan mereka dapat dikeluarkan dari model jika isinya dikonfirmasi dan penjelasan logis diberikan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfOebX9Ewot4"
      },
      "source": [
        "# Deteksi fitur yang tidak pnting (unimportant features)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOxo9nZkwyR4"
      },
      "source": [
        "forest_importance = pd.merge(forest_importance1.drop(\"cumsum\", axis=1), forest_importance2.drop(\"cumsum\", axis=1),\n",
        "                             left_on=\"variables\", right_on=\"variables\", how='left')\n",
        "forest_importance"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_IEE6j6w5Fr"
      },
      "source": [
        "\n",
        "Kedua kepentingan menghitung jumlah masing-masing rasio."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jlg-hMDow8Lc"
      },
      "source": [
        "count = []\n",
        "ratio = []\n",
        "for i in range(0,11):\n",
        "    r = i*0.1\n",
        "    c = forest_importance[(forest_importance[\"feature_importance1\"]<=i*0.1) & (forest_importance[\"feature_importance2\"]<=i*0.1)][\"variables\"].count()\n",
        "    count.append(c)\n",
        "    ratio.append(r)\n",
        "    \n",
        "pd.DataFrame({\"Both ratio(%)<\":ratio,\n",
        "              \"count\":count})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i7uexFglxDK8"
      },
      "source": [
        "Mereka yang memiliki kedua kepentingan di bawah ambang batas masing-masing dihitung. Hasilnya, ada 13 variabel dengan kedua kepentingan sama-sama 0. Selain itu, 27 variabel sesuai dengan ambang 0,1% dan 42 variabel sesuai dengan ambang 0,2%.\n",
        "\n",
        "Selanjutnya, hasil kinerja klasifikasi dengan model hutan acak ketika variabel dijatuhkan untuk setiap ambang batas dibandingkan.\n",
        "Dari hasil keseluruhan data dan hasil cluster, variabel optimum dapat berubah tergantung kepentingannya, sehingga kondisi diset menjadi “atau”."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xPF6-ymKxFng"
      },
      "source": [
        "# Verifikasi dengan seluruh data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooXjdU8KxEjt"
      },
      "source": [
        "# Variables\n",
        "X = train_std.iloc[:,:-2]\n",
        "Y = train_std[\"class\"]\n",
        "\n",
        "# Roop\n",
        "Threshold = []\n",
        "col_count = []\n",
        "accuracy = []\n",
        "precision = []\n",
        "recall = []\n",
        "f1_ = []\n",
        "\n",
        "for i in range(0,10):\n",
        "    # Create instance, parameters are default.\n",
        "    forest  = RandomForestClassifier(n_estimators=10, random_state=10)\n",
        "    \n",
        "    # threshold\n",
        "    thre = \"> 0.\" + str(i) +\"%\"\n",
        "\n",
        "    col = forest_importance[(forest_importance[\"feature_importance1\"] >= i*0.1) | (forest_importance[\"feature_importance2\"] >= i*0.1)][\"variables\"].values\n",
        "    col_c = len(col)\n",
        "    # Select data\n",
        "    X = train_std[col]\n",
        "    y = train_std[\"class\"]\n",
        "    # With over sampling\n",
        "    # Data preprocessing, oversampling method\n",
        "    # create instance\n",
        "    ros = RandomOverSampler(sampling_strategy=\"auto\", random_state=10)\n",
        "\n",
        "    # train test data split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n",
        "\n",
        "    # Apply to data\n",
        "    X_resampled, y_resampled = ros.fit_sample(X_train, y_train)\n",
        "    \n",
        "    # Fitting\n",
        "    forest.fit(X_resampled, y_resampled)\n",
        "    \n",
        "    # Prediction\n",
        "    y_pred = forest.predict(X_test)\n",
        "\n",
        "    # Scores\n",
        "    acc = accuracy_score(y_true=y_test, y_pred=y_pred).round(3)\n",
        "    pre = precision_score(y_true=y_test, y_pred=y_pred).round(3)\n",
        "    rec = recall_score(y_true=y_test, y_pred=y_pred).round(3)\n",
        "    f1 = f1_score(y_true=y_test, y_pred=y_pred).round(3)\n",
        "    \n",
        "    # list append\n",
        "    Threshold.append(thre)\n",
        "    col_count.append(col_c)\n",
        "    accuracy.append(acc)\n",
        "    precision.append(pre)\n",
        "    recall.append(rec)\n",
        "    f1_.append(f1)\n",
        "\n",
        "# create dataframe\n",
        "pd.DataFrame({\"Threshold\":Threshold,\n",
        "              \"Col_count\":col_count,\n",
        "             \"accuracy\":accuracy,\n",
        "             \"precision\":precision,\n",
        "             \"recall\":recall,\n",
        "             \"f1_score\":f1_})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Z5dgPyJxTPk"
      },
      "source": [
        "# verifikasi seluruh data dan hasil clustering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbhqS3Xmxe0d"
      },
      "source": [
        "# Variables\n",
        "X = train_std.iloc[:,:-2]\n",
        "Y = train_std[\"class\"]\n",
        "\n",
        "# Roop\n",
        "Threshold = []\n",
        "col_count = []\n",
        "accuracy = []\n",
        "precision = []\n",
        "recall = []\n",
        "f1_ = []\n",
        "\n",
        "for i in range(0,10):\n",
        "    # Create instance, parameters are default.\n",
        "    forest  = RandomForestClassifier(n_estimators=10, random_state=10)\n",
        "    \n",
        "    # threshold\n",
        "    thre = \"> 0.\" + str(i) +\"%\"\n",
        "\n",
        "    col = forest_importance[(forest_importance[\"feature_importance1\"] >= i*0.1) | (forest_importance[\"feature_importance2\"] >= i*0.1)][\"variables\"].values\n",
        "    col_c = len(col)\n",
        "    # Select data\n",
        "    X = train_std.query(\"cluster!=0 & cluster!=4\")[col]\n",
        "    y = train_std.query(\"cluster!=0 & cluster!=4\")[\"class\"]\n",
        "\n",
        "    # train test data split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
        "    \n",
        "    # Fitting\n",
        "    forest.fit(X_train, y_train)\n",
        "    \n",
        "    # Prediction\n",
        "    y_pred = forest.predict(X_test)\n",
        "\n",
        "    # Scores\n",
        "    acc = accuracy_score(y_true=y_test, y_pred=y_pred).round(3)\n",
        "    pre = precision_score(y_true=y_test, y_pred=y_pred).round(3)\n",
        "    rec = recall_score(y_true=y_test, y_pred=y_pred).round(3)\n",
        "    f1 = f1_score(y_true=y_test, y_pred=y_pred).round(3)\n",
        "    \n",
        "    # list append\n",
        "    Threshold.append(thre)\n",
        "    col_count.append(col_c)\n",
        "    accuracy.append(acc)\n",
        "    precision.append(pre)\n",
        "    recall.append(rec)\n",
        "    f1_.append(f1)\n",
        "\n",
        "# create dataframe\n",
        "pd.DataFrame({\"Threshold\":Threshold,\n",
        "              \"Col_count\":col_count,\n",
        "             \"accuracy\":accuracy,\n",
        "             \"precision\":precision,\n",
        "             \"recall\":recall,\n",
        "             \"f1_score\":f1_})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ma5_C43TxjQD"
      },
      "source": [
        "Melihat hasil menggunakan salah satu data pembelajaran, dapat dilihat bahwa pengurangan variabel tidak secara seragam mengurangi kinerja. Melihat kedua hasil tersebut, ditemukan bahwa hasil dengan threshold 0,6% adalah yang terbaik."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BKn9UB7zxmU9"
      },
      "source": [
        "best_col = forest_importance[(forest_importance[\"feature_importance1\"] >= 0.6) | (forest_importance[\"feature_importance2\"] >= 0.6)][\"variables\"].values\n",
        "print(best_col)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ZOY5NSGy931"
      },
      "source": [
        "# Analisis Fitur Penting menggunakan Random forest dan PCA variables"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NixS61CgzI4I"
      },
      "source": [
        "Selanjutnya, pentingnya sumbu komponen utama dianalisis dengan cara yang sama menggunakan hasil PCA di mana jumlah variabel dikurangi dengan pengurangan dimensi."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DxDOWC8xzINJ"
      },
      "source": [
        "# Difine variables\n",
        "X = pca_result.iloc[:,:-2]\n",
        "Y = pca_result[\"class\"]\n",
        "\n",
        "# train test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n",
        "\n",
        "# Data preprocessing, oversampling method\n",
        "# create instance\n",
        "ros = RandomOverSampler(sampling_strategy=\"auto\", random_state=10)\n",
        "\n",
        "# Apply to data\n",
        "X_resampled, y_resampled = ros.fit_sample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8SAertlzOoQ"
      },
      "source": [
        "# Create instance\n",
        "forest  = RandomForestClassifier(n_estimators=10, random_state=10)\n",
        "\n",
        "# parameters\n",
        "param_range = [20, 25, 30]\n",
        "leaf = [85, 90, 95, 100]\n",
        "criterion = [\"entropy\", \"gini\", \"error\"]\n",
        "param_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n",
        "\n",
        "# Optimization by Grid search, scoring is f1\n",
        "gs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\n",
        "gs = gs.fit(X_resampled, y_resampled)\n",
        "\n",
        "print(\"gs best:%.3f\" % gs.best_score_)\n",
        "print(\"gs params:{}\".format(gs.best_params_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBOTXrp4zTSC"
      },
      "source": [
        "# Prediction\n",
        "gs_best = gs.best_estimator_\n",
        "\n",
        "y_pred = gs_best.predict(X_test)\n",
        "\n",
        "# Scores\n",
        "print(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
        "print(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZnDpZB2zXOw"
      },
      "source": [
        "# Create best model of random forest classifier\n",
        "forest  = RandomForestClassifier(n_estimators=10, random_state=10, criterion=\"entropy\", max_depth=15, max_leaf_nodes=60)\n",
        "forest.fit(X_resampled, y_resampled)\n",
        "\n",
        "importance = forest.feature_importances_\n",
        "\n",
        "indices = np.argsort(importance)[::-1]\n",
        "\n",
        "# Karena banyaknya item, hanya 10 teratas yang ditulis.\n",
        "for i in range(10):\n",
        "    print(\"%2d) %-*s %f\" %(i+1, 10, X.columns[indices[i]], importance[indices[i]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J1-EbRvzaaR"
      },
      "source": [
        "# Visualization with paret0 graph\n",
        "forest_importance = pd.DataFrame({})\n",
        "variables = []\n",
        "feature_importance = []\n",
        "for i in range(len(indices)):\n",
        "    col = X.columns[indices[i]]\n",
        "    impor = importance[indices[i]]\n",
        "    variables.append(col)\n",
        "    feature_importance.append(impor)\n",
        "forest_importance[\"variables\"] = variables\n",
        "forest_importance[\"feature_importance\"] = feature_importance\n",
        "forest_importance[\"feature_importance\"] = forest_importance[\"feature_importance\"]*100\n",
        "forest_importance[\"cumsum\"] = forest_importance[\"feature_importance\"].cumsum()\n",
        "\n",
        "\n",
        "# Graph\n",
        "fig, ax1 = plt.subplots(figsize=(20,8))\n",
        "ax1.bar(forest_importance[\"variables\"], forest_importance[\"feature_importance\"], label=\"importance\")\n",
        "ax1.grid()\n",
        "ax1.set_xlabel(\"variables\")\n",
        "ax1.tick_params(axis='x', rotation=90, labelsize=10)\n",
        "ax1.set_ylabel(\"importance(%)\")\n",
        "plt.legend(loc='lower left')\n",
        "ax2 = ax1.twinx()\n",
        "ax2.plot(forest_importance[\"variables\"], forest_importance[\"cumsum\"], color=\"red\", label=\"ratio\")\n",
        "ax2.set_ylim([0,110])\n",
        "ax2.set_ylabel(\"Ratio(%)\")\n",
        "plt.legend(loc=\"upper right\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q5awUnJXzeED"
      },
      "source": [
        "Hasil analisis komponen utama menunjukkan tidak ada perbedaan kinerja klasifikasi yang signifikan dari hasil analisis menggunakan seluruh data. Tetapi yang penting adalah bahwa beberapa informasi dikurangi ketika memilih jumlah komponen utama. Sangat berguna untuk mendapatkan hasil yang serupa meskipun demikian, karena mudah untuk memahami keseluruhan gambar untuk analisis.\n",
        "\n",
        "Meskipun hanya sekitar 1%, namun kecenderungan over-fitting pada data uji lebih kecil dibandingkan dengan data latih. Hal ini dapat dilihat karena jumlah variabel telah berkurang dan model menjadi lebih kuat. Metode berbasis PCA ini lebih cocok untuk operasi aktual dan prediksi data yang tidak diketahui."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5kJMpnezwMs"
      },
      "source": [
        "#Membandingkan metode pengambilan sampel¶"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FBjOWXYvmuWj"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "Dari hasil analisis selama ini, ditemukan pentingnya membangun model dengan menggunakan seluruh data. Dalam hal ini, perlu diterapkan metode pengambilan sampel pada data yang tidak seimbang. Mulai sekarang, kami akan mengkonfirmasi efek dari beberapa metode ini dan mempertimbangkan yang terbaik. Prediksi menggunakan model hutan acak.\n",
        "\n",
        "Metode yang dikonfirmasi adalah sebagai berikut:\n",
        "Over sampling (Already confirmed)\n",
        "Under sampling\n",
        "SMOTE method\n",
        "NearMiss method\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z14Ipgnrz31R"
      },
      "source": [
        "**Under sampling**\n",
        "Konfirmasikan hasil metode Under sampling di bawah kondisi yang sama seperti Oversampling, yang menangani semua data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RQTcCPiMz_sU"
      },
      "source": [
        "# Difine variables\n",
        "X = train_std.iloc[:,:-2]\n",
        "Y = train_std[\"class\"]\n",
        "\n",
        "# Data preprocessing, oversampling method\n",
        "# create instance\n",
        "rus = RandomUnderSampler(sampling_strategy=\"auto\", random_state=10)\n",
        "\n",
        "# train test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n",
        "\n",
        "# Apply to data\n",
        "X_resampled, y_resampled = rus.fit_sample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66U9FEJC0DsS"
      },
      "source": [
        "# Create instance\n",
        "forest  = RandomForestClassifier(n_estimators=10, random_state=10)\n",
        "\n",
        "# parameters\n",
        "param_range = [5, 10,15,20]\n",
        "leaf = [60, 65, 70, 75]\n",
        "criterion = [\"entropy\", \"gini\", \"error\"]\n",
        "param_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n",
        "\n",
        "# Optimization by Grid search, scoring is f1 score\n",
        "gs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\n",
        "gs = gs.fit(X_resampled, y_resampled)\n",
        "\n",
        "print(\"gs best:%.3f\" % gs.best_score_)\n",
        "print(\"gs params:{}\".format(gs.best_params_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yiz006mk0HkV"
      },
      "source": [
        "# Prediction\n",
        "gs_best = gs.best_estimator_\n",
        "\n",
        "y_pred = gs_best.predict(X_test)\n",
        "\n",
        "# Scores\n",
        "print(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
        "print(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jnM-v920XK_"
      },
      "source": [
        "**SMOTE**\n",
        "mengonfirmasi metode SMOTE"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NdKffZr60eab"
      },
      "source": [
        "# Difine variables\n",
        "X = train_std.iloc[:,:-2]\n",
        "Y = train_std[\"class\"]\n",
        "\n",
        "# Data preprocessing, oversampling method\n",
        "# create instance\n",
        "smote = SMOTE(sampling_strategy=\"auto\", random_state=10)\n",
        "\n",
        "# train test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n",
        "\n",
        "# Apply to data\n",
        "X_resampled, y_resampled = smote.fit_sample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kElV2Wf40iw1"
      },
      "source": [
        "# Create instance\n",
        "forest  = RandomForestClassifier(n_estimators=10, random_state=10)\n",
        "\n",
        "# parameters\n",
        "param_range = [10,15,20]\n",
        "leaf = [80, 85, 90, 95]\n",
        "criterion = [\"entropy\", \"gini\", \"error\"]\n",
        "param_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n",
        "\n",
        "# Optimization by Grid search, scoring is f1 score\n",
        "gs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\n",
        "gs = gs.fit(X_resampled, y_resampled)\n",
        "\n",
        "print(\"gs best:%.3f\" % gs.best_score_)\n",
        "print(\"gs params:{}\".format(gs.best_params_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0vgrtmo0lw6"
      },
      "source": [
        "# Prediction\n",
        "gs_best = gs.best_estimator_\n",
        "\n",
        "y_pred = gs_best.predict(X_test)\n",
        "\n",
        "# Scores\n",
        "print(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
        "print(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "haJsKt300nSW"
      },
      "source": [
        "**NearMiss method**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAuWBP8C0yAG"
      },
      "source": [
        "# Difine variables\n",
        "X = train_std.iloc[:,:-2]\n",
        "Y = train_std[\"class\"]\n",
        "\n",
        "# Data preprocessing, oversampling method\n",
        "# create instance\n",
        "nem = NearMiss(sampling_strategy=\"auto\")\n",
        "\n",
        "# train test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=10)\n",
        "\n",
        "# Apply to data\n",
        "X_resampled, y_resampled = nem.fit_sample(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ypbXHC_01S_"
      },
      "source": [
        "# Create instance\n",
        "forest  = RandomForestClassifier(n_estimators=10, random_state=10)\n",
        "\n",
        "# parameters\n",
        "param_range = [5, 10,15,20]\n",
        "leaf = [60, 65, 70, 75]\n",
        "criterion = [\"entropy\", \"gini\", \"error\"]\n",
        "param_grid = [{\"max_depth\":param_range, \"criterion\":criterion, \"max_leaf_nodes\":leaf}]\n",
        "\n",
        "# Optimization by Grid search, scoring is f1 score\n",
        "gs = GridSearchCV(estimator=forest, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\n",
        "gs = gs.fit(X_resampled, y_resampled)\n",
        "\n",
        "print(\"gs best:%.3f\" % gs.best_score_)\n",
        "print(\"gs params:{}\".format(gs.best_params_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Npcp0ey304CM"
      },
      "source": [
        "# Prediction\n",
        "gs_best = gs.best_estimator_\n",
        "\n",
        "y_pred = gs_best.predict(X_test)\n",
        "\n",
        "# Scores\n",
        "print(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_test, y_pred=y_pred))\n",
        "print(\"accuracy = %.3f\" % accuracy_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"precision = %.3f\" % precision_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"recall = %.3f\" % recall_score(y_true=y_test, y_pred=y_pred))\n",
        "print(\"f1_score = %.3f\" % f1_score(y_true=y_test, y_pred=y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZGqDNAHY06gY"
      },
      "source": [
        "Kami mengkonfirmasi metode pengambilan sampel untuk data yang tidak seimbang dalam kondisi yang sama. Jika dibandingkan dengan skor f1 untuk melihat keseimbangan antara presisi dan recall, metode over sampling memberikan hasil terbaik untuk data ini. Di sisi lain, undersampling dan turunannya, metode Near Miss, memiliki nilai recall yang tinggi tetapi nilai presisi yang rendah. Ini mungkin karena kekuatan negatif dari negatif dikurangi dengan mengurangi jumlah kelas negatif dan harus dihindari untuk data ini."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5fHpIPuH0-vC"
      },
      "source": [
        "# Model Pengklasifikasi berdasarkan hasil sejauh ini\n",
        "\n",
        "Berdasarkan hasil analisis sampai saat ini, kami memutuskan untuk mengambil kebijakan berikut mengenai penanganan data variabel dalam klasifikasi.\n",
        "\n",
        "(1) Kurangi jumlah variabel yang tidak penting, dengan nilai threshold fitur penting 0,6%. Saya dapat mengharapkan efek pada PCA juga, tetapi kali ini saya memutuskan untuk melanjutkan dengan pengurangan variabel berdasarkan kepentingan. (Mungkin kita bisa mengharapkan peningkatan lebih lanjut dengan menggabungkannya ...)\n",
        "(2) Analisis klaster dilakukan pada variabel yang direduksi untuk membuat kelompok klaster yang kelas positif dan negatifnya relatif seimbang.\n",
        "(3) Latih model prediktif pada seluruh data dan data klaster yang disempurnakan dan gabungkan hasilnya. Untuk model yang menangani seluruh data, metode pengambilan sampel untuk data tidak seimbang adalah over sampling yang digunakan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oiA1h1qL1Wkk"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V5nSsBxL1bQM"
      },
      "source": [
        "# Classification prediction model¶\n",
        "\n",
        "**Prediction model : I use LGBM classifier**\n",
        "**Difine class and function, k-fold cross validation, learning and validation curve, confusion matrix, roc auc curve**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PtobSq31qPT"
      },
      "source": [
        "class k_fold_cross_val:\n",
        "    def __init__(self, X_train, y_train, estimator, cv):\n",
        "        self.X_train = X_train\n",
        "        self.y_train = y_train\n",
        "        self.estimator = estimator\n",
        "        self.cv = cv\n",
        "        \n",
        "    def cross_val_kfold(self):\n",
        "        kfold = StratifiedKFold(n_splits=self.cv, random_state=10)\n",
        "        self.kfold = kfold\n",
        "        \n",
        "        scores = []\n",
        "        for train_idx, test_idx in self.kfold.split(self.X_train, self.y_train):\n",
        "            self.estimator.fit(self.X_train[train_idx], self.y_train.values[train_idx])\n",
        "            score = self.estimator.score(self.X_train[test_idx], self.y_train.values[test_idx])\n",
        "            scores.append(score)\n",
        "            print(\"Class: %s, Acc: %.3f\" % (np.bincount(self.y_train.values[train_idx]), score))\n",
        "            self.scores = scores\n",
        "            \n",
        "    def score(self):\n",
        "        scores = cross_val_score(estimator=self.estimator, X=self.X_train, y=self.y_train, cv=self.cv, n_jobs=1)\n",
        "        print(\"CV accuracy scores: %s\" % self.scores)\n",
        "        print(\"CV accuracy: %.3f +/- %.3f\" % (np.mean(self.scores), np.std(self.scores)))\n",
        "        \n",
        "    def draw_roc_curve(self, X_test, y_test):\n",
        "        self.X_test = X_test\n",
        "        self.y_test = y_test\n",
        "        \n",
        "        mean_tpr=0\n",
        "        mean_fpr=np.linspace(0,1,100)\n",
        "        plt.figure(figsize=(10,6))\n",
        "        for train_idx, test_idx in self.kfold.split(self.X_train, self.y_train):\n",
        "            proba = self.estimator.fit(self.X_train[train_idx], self.y_train.values[train_idx]).predict_proba(self.X_train[test_idx])\n",
        "            fpr, tpr, thresholds = roc_curve(y_true=self.y_train.values[test_idx], y_score=proba[:,1], pos_label=1)\n",
        "            mean_tpr += interp(mean_fpr, fpr, tpr)\n",
        "            mean_tpr[0] = 0\n",
        "            roc_auc = auc(fpr, tpr)\n",
        "            plt.plot(fpr, tpr, lw=1, label=\"ROC fold (area=%.2f)\" %(roc_auc))\n",
        "        \n",
        "        # Line\n",
        "        plt.plot([0,1], [0,1], linestyle='--', color=(0.6,0.6,0.6), label=\"random guessing\")\n",
        "        # plot mean of fpr, tpr roc_auc\n",
        "        mean_tpr /= self.cv\n",
        "        mean_tpr[-1] = 1.0\n",
        "        mean_auc = auc(mean_fpr, mean_tpr)\n",
        "        plt.plot(mean_fpr, mean_tpr, 'k--', label=\"mean ROC (area = %.2f)\" % mean_auc, color=\"blue\")\n",
        "        # Line\n",
        "        plt.plot([0,0,1], [0,1,1], lw=2, linestyle=':', color=\"black\", label='perfect performance')\n",
        "        plt.xlabel(\"false positive rate\")\n",
        "        plt.ylabel(\"true positive rate\")\n",
        "        plt.title(\"Receiver Operator Characteristic\")\n",
        "        plt.legend(loc=\"upper right\")\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vkLTSwl4Zoi"
      },
      "source": [
        "def draw_learning_curve(estimator, X_train, y_train):\n",
        "    # learning curve\n",
        "    train_sizes, train_scores, test_scores = learning_curve(estimator=estimator, X=X_train, y=y_train, train_sizes=np.linspace(0.1,1,10), cv=10, n_jobs=1)\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    test_mean = np.mean(test_scores, axis=1)\n",
        "    test_std = np.std(test_scores, axis=1)\n",
        "\n",
        "    # plot\n",
        "    plt.figure(figsize=(10,6))\n",
        "    # train data\n",
        "    plt.plot(train_sizes, train_mean, color=\"blue\", marker='o', markersize=5, label='training accuracy')\n",
        "    plt.fill_between(train_sizes, train_mean+train_std, train_mean-train_std, color=\"blue\", alpha=0.15)\n",
        "    # val data\n",
        "    plt.plot(train_sizes, test_mean, color=\"green\", marker='s', linestyle='--', markersize=5, label='validation accuracy')\n",
        "    plt.fill_between(train_sizes, test_mean+test_std, test_mean-test_std, color=\"green\", alpha=0.15)\n",
        "\n",
        "    plt.grid()\n",
        "    plt.xlabel(\"Number of trainig samples\")\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.ylim([0.8,1.0])\n",
        "    plt.title(\"Learning curve\")\n",
        "    plt.legend(loc=\"upper right\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsnHAJgO1wWr"
      },
      "source": [
        "def draw_validation_curve(estimator, X_train, y_train, param_name, param_range, xscale):\n",
        "    # validation curve\n",
        "    train_scores, test_scores = validation_curve(estimator=estimator, X=X_train, y=y_train, param_name=param_name, param_range=param_range, cv=10)\n",
        "    train_mean = np.mean(train_scores, axis=1)\n",
        "    train_std = np.std(train_scores, axis=1)\n",
        "    test_mean = np.mean(test_scores, axis=1)\n",
        "    test_std = np.std(test_scores, axis=1)\n",
        "    \n",
        "    # plot\n",
        "    plt.figure(figsize=(10,6))\n",
        "    # train data\n",
        "    plt.plot(param_range, train_mean, color=\"blue\", marker='o', markersize=5, label='training accuracy')\n",
        "    plt.fill_between(param_range, train_mean+train_std, train_mean-train_std, color=\"blue\", alpha=0.15)\n",
        "    # val data\n",
        "    plt.plot(param_range, test_mean, color=\"green\", marker='s', linestyle='--', markersize=5, label='validation accuracy')\n",
        "    plt.fill_between(param_range, test_mean+test_std, test_mean-test_std, color=\"green\", alpha=0.15)\n",
        "\n",
        "    plt.grid()\n",
        "    plt.xlabel(\"{}\".format(param_name))\n",
        "    if xscale==\"log\":\n",
        "        plt.xscale(\"log\")\n",
        "    else:\n",
        "        pass\n",
        "    plt.ylabel(\"Accuracy\")\n",
        "    plt.ylim([0.8,1.0])\n",
        "    plt.title(\"Validation curve\")\n",
        "    plt.legend(loc=\"upper right\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pkzX-fut13B0"
      },
      "source": [
        "**Parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9Ce0vMTk17ex"
      },
      "source": [
        "best_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxkOczbb1_6N"
      },
      "source": [
        "#1st Prediksi Seluruh Data\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sBWyCsqx2Jq4"
      },
      "source": [
        "# Selected parameters, feature importance threshold >=0.6\n",
        "col = best_col\n",
        "# Variables\n",
        "X = train_std[col]\n",
        "y = train_std[\"class\"]\n",
        "\n",
        "# train test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
        "\n",
        "# Apply over sampling method\n",
        "ros = RandomOverSampler(sampling_strategy=\"auto\", random_state=10)\n",
        "X_resampled, y_resampled = ros.fit_sample(X_train, y_train)\n",
        "\n",
        "# Study best parameter by Cross validation\n",
        "# Instance\n",
        "lgb_ = lgb.LGBMClassifier()\n",
        "\n",
        "# prameters\n",
        "max_depth = [5, 10, 15]\n",
        "min_samples_leaf = [1,3,5,7]\n",
        "min_samples_split = [4,6, 8, 10]\n",
        "\n",
        "param_grid = [{\"max_depth\":max_depth,\n",
        "               \"min_samples_leaf\":min_samples_leaf, \"min_samples_split\":min_samples_split}]\n",
        "\n",
        "# Optimization by Grid search\n",
        "gs = GridSearchCV(estimator=lgb_, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\n",
        "gs = gs.fit(X_resampled, y_resampled)\n",
        "\n",
        "print(\"gs best:%.3f\" % gs.best_score_)\n",
        "print(\"gs params:{}\".format(gs.best_params_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r7GE02FQ2O8C"
      },
      "source": [
        " # best params\n",
        "gs_best_all = gs.best_estimator_\n",
        "\n",
        "# Cross validation\n",
        "cv = k_fold_cross_val(X_resampled.values, y_resampled, gs_best_all, 5)\n",
        "cv.cross_val_kfold()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2dIsnrU2Rz0"
      },
      "source": [
        "# cross val score\n",
        "cv.score()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rz1CPkH42TsU"
      },
      "source": [
        "# learning curve\n",
        "draw_learning_curve(gs_best_all, X_resampled, y_resampled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ByqYJlQv2WS-"
      },
      "source": [
        "# validation curve\n",
        "draw_validation_curve(gs_best_all, X_resampled, y_resampled, \"max_depth\", param_range, \"\"), X_resampled, y_resampled, \"max_depth\", param_range, \"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25GwYddf2YEG"
      },
      "source": [
        "# cv training roc curve\n",
        "cv.draw_roc_curve(X_resampled, y_resampled)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrCpQ78a2cJI"
      },
      "source": [
        "# test data prediction\n",
        "y_pred_all = gs_best_all.predict(X_test)\n",
        "\n",
        "# Confusion matrix and ROC curve\n",
        "confmat_roccurve(X_test, y_test, y_pred_all, gs_best_all)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ROjQsFk2huk"
      },
      "source": [
        "# 2. Dengan prediksi data analisis cluster"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lHRdcKPr21Qu"
      },
      "source": [
        "# Selected parameters, feature importance threshold >=0.6\n",
        "col = best_col\n",
        "# Variables\n",
        "X = train_std.query(\"cluster!=0 & cluster!=4\")[col]\n",
        "y = train_std.query(\"cluster!=0 & cluster!=4\")[\"class\"]\n",
        "\n",
        "# train test data split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=10)\n",
        "\n",
        "# Apply over sampling method\n",
        "ros = RandomOverSampler(sampling_strategy=\"auto\", random_state=10)\n",
        "X_resampled, y_resampled = ros.fit_sample(X_train, y_train)\n",
        "\n",
        "# Study best parameter by Cross validation\n",
        "# Instance\n",
        "lgb_ = lgb.LGBMClassifier()\n",
        "\n",
        "# prameters\n",
        "max_depth = [5, 10, 15]\n",
        "min_samples_leaf = [1,3,5,7]\n",
        "min_samples_split = [4,6, 8, 10]\n",
        "\n",
        "param_grid = [{\"max_depth\":max_depth,\n",
        "               \"min_samples_leaf\":min_samples_leaf, \"min_samples_split\":min_samples_split}]\n",
        "\n",
        "# Optimization by Grid search\n",
        "gs = GridSearchCV(estimator=lgb_, param_grid=param_grid, scoring=\"f1\", cv=5, n_jobs=-1)\n",
        "gs = gs.fit(X_resampled, y_resampled)\n",
        "\n",
        "print(\"gs best:%.3f\" % gs.best_score_)\n",
        "print(\"gs params:{}\".format(gs.best_params_))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RjLIxX_27E-"
      },
      "source": [
        "# best params\n",
        "gs_best_cluster = gs.best_estimator_\n",
        "\n",
        "# Cross validation\n",
        "cv = k_fold_cross_val(X_resampled.values, y_resampled, gs_best_cluster, 5)\n",
        "cv.cross_val_kfold()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wWbSUwA629QE"
      },
      "source": [
        "# cross val score\n",
        "cv.score()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p5fm6RQ62_Wi"
      },
      "source": [
        "# learning curve\n",
        "draw_learning_curve(gs_best_cluster, X_resampled, y_resampled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2aGfToWX3Bxn"
      },
      "source": [
        "# validation curve\n",
        "draw_validation_curve(gs_best_cluster, X_resampled, y_resampled, \"max_depth\", param_range, \"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtWkBmkx3ECi"
      },
      "source": [
        "# cv training roc curve\n",
        "cv.draw_roc_curve(X_resampled, y_resampled)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r_ADabQG3HEb"
      },
      "source": [
        "# test data prediction\n",
        "y_pred_cluster = gs_best_cluster.predict(X_test)\n",
        "\n",
        "# Confusion matrix and ROC curve\n",
        "confmat_roccurve(X_test, y_test, y_pred_cluster, gs_best_cluster)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E1qi0GLU3Mti"
      },
      "source": [
        "# Pengujian prediksi data Buat prediksi ensemble¶\n",
        "\n",
        "**Preprocessing** \n",
        "Seluruh data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I66S-wCJ3YA3"
      },
      "source": [
        "# Selected parameters, feature importance threshold >=0.6\n",
        "col = best_col\n",
        "# Variables\n",
        "X_Test = test_std[col]\n",
        "y_Test = test_std[\"class\"]\n",
        "\n",
        "# test data prediction\n",
        "y_Test_pred_all = gs_best_all.predict(X_Test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SRsk6tUh3sUf"
      },
      "source": [
        "**custer data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0stkPeSF3vBx"
      },
      "source": [
        "# Variables\n",
        "X_Test_cluster = test_std.query(\"cluster!=0 & cluster!=4\")[col]\n",
        "y_Test_cluster = test_std.query(\"cluster!=0 & cluster!=4\")[\"class\"]\n",
        "\n",
        "# test data prediction\n",
        "y_Test_pred_cluster = gs_best_cluster.predict(X_Test_cluster)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vooe-QMq30No"
      },
      "source": [
        "# Perbandingan setiap hasil prediksi"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1YyjA60J31-9"
      },
      "source": [
        "# Scores\n",
        "# All data prediction\n",
        "print(\"-\"*30, \"all data\", \"-\"*30)\n",
        "print(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_Test, y_pred=y_Test_pred_all))\n",
        "print(\"accuracy = %.3f\" % accuracy_score(y_true=y_Test, y_pred=y_Test_pred_all))\n",
        "print(\"precision = %.3f\" % precision_score(y_true=y_Test, y_pred=y_Test_pred_all))\n",
        "print(\"recall = %.3f\" % recall_score(y_true=y_Test, y_pred=y_Test_pred_all))\n",
        "print(\"f1_score = %.3f\" % f1_score(y_true=y_Test, y_pred=y_Test_pred_all))\n",
        "\n",
        "# with cluster data prediction\n",
        "print(\"-\"*30, \"cluster data\", \"-\"*30)\n",
        "print(\"Confusion_matrix = \\n\", confusion_matrix(y_true=y_Test_cluster, y_pred=y_Test_pred_cluster))\n",
        "print(\"accuracy = %.3f\" % accuracy_score(y_true=y_Test_cluster, y_pred=y_Test_pred_cluster))\n",
        "print(\"precision = %.3f\" % precision_score(y_true=y_Test_cluster, y_pred=y_Test_pred_cluster))\n",
        "print(\"recall = %.3f\" % recall_score(y_true=y_Test_cluster, y_pred=y_Test_pred_cluster))\n",
        "print(\"f1_score = %.3f\" % f1_score(y_true=y_Test_cluster, y_pred=y_Test_pred_cluster))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWrrJEc_39ir"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clmMF0sJ3_LC"
      },
      "source": [
        "# Kesimpulan\n",
        "\n",
        "Hasilnya sesuai dengan kedua baris ke data pelatihan, tetapi tidak berhasil mendeteksi positif ke data uji. Hasilnya adalah ada kecenderungan yang kuat untuk belajar berlebihan.\n",
        "Hipotesis saya dan tindakan balasan sebagai penyebabnya adalah\n",
        "\n",
        "Bukankah variabel data uji tidak terkandung dalam data pelatihan di ruang dimensi tinggi dan ada banyak elemen di luar? Oleh karena itu, ada kemungkinan bahwa bahkan variabel penting berkurang ketika variabel dikurangi. Penanggulangan: Turunkan ambang batas pengurangan variabel.\n",
        "Masih banyak variabel, dan beberapa variabel dalam data latih yang memiliki pengaruh kuat terlalu kuat dalam data uji. Penanggulangan: Menyamakan pengaruh variabel dengan analisis komponen utama.\n",
        "Untuk uji coba selanjutnya, saya akan mencoba dua di atas."
      ]
    }
  ]
}